<html>
<head>
<title>Computer Vision Project</title>
<link href='http://fonts.googleapis.com/css?family=Nunito:300|Crimson+Text|Droid+Sans+Mono' rel='stylesheet' type='text/css'>
<link rel="stylesheet" title="Default" href="styles/github.css">
<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js"></script>

<link rel="stylesheet" href="highlighting/styles/default.css">
<script src="highlighting/highlight.pack.js"></script>

<style type="text/css">
body {
	margin: 0px;
	width: 100%;
	font-family: 'Crimson Text', serif;
	font-size: 20px;
	background: #fcfcfc;
}
h1 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 28px;
	margin: 25px 0px 0px 0px;
	text-transform: lowercase;

}

h2 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 32px;
	margin: 15px 0px 35px 0px;
	color: #333;
	word-spacing: 3px;
}

h3 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 26px;
	margin: 10px 0px 10px 0px;
	color: #333;
	word-spacing: 2px;
}
h4 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 22px;
	margin: 10px 0px 10px 0px;
	color: #333;
	word-spacing: 2px;
}

h5 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 18px;
	margin: 10px 0px 10px 0px;
	color: #111;
	word-spacing: 2px;
}

p, li {
	color: #444;
}

a {
	color: #DE3737;
}

.container {
	margin: 0px auto 0px auto;
	width: 960px;
}

#header {
	background: #333;
	width: 100%;
}

#headersub {
	color: #ccc;
	width: 960px;
	margin: 0px auto 0px auto;
	padding: 20px 0px 20px 0px;
}

.chart {
	width: 480px;
}
.lol {
	font-size: 16px;
	color: #888;
	font-style: italic;
}
.sep {
	height: 1px;
	width: 100%;
	background: #999;
	margin: 20px 0px 20px 0px;
}
.footer{
	font-size: 16px;
}
.latex {
	width: 100%;
}

.latex img {
	display: block;
	margin: 0px auto 0px auto;
}

pre {
	font-family: 'Droid Sans Mono';
	font-size: 14px;
}

td img {
  vertical-align: middle;
}

#contents a {
}
</style>
<script type="text/javascript">
    hljs.initHighlightingOnLoad();
</script>
</head>
<body>
<div id="header" >
<div id="headersub">
<h1><span style="color: #DE3737">Christopher Fleisher</span></h1>
</div>
</div>
<div class="container">

<h2>Project 1: Image Filtering and Hybrid Images</h2>

<div>
	<p>The general structure used for generating hybrid images consisted of the following three phases:</p>
	<ol>
	<li>Setup - standardizes image framing process based on provided images and filter dimensions</li>
	<li>Filtering - executes filter on provided images</li>
	<li>Composition - recombines high and low frequencies from images to generate final hybrid image</li>
	</ol>
</div>

<div>
	<h3>Setup</h3>
	<p>The primary focus of the setup phase is to standardize the image framing process. This is achieved by buffering each edge of the provided image based on the provided filter's dimensions. For example, if the provided filter is a 29x29 matrix, then the buffer size gets set at 29 and the image's color channel edges are framed by a similarly sized 29x29 buffer. I elected to symmetrically pad the image utilizing the numpy pad function after initially framing the image by hand with a black buffer (womp womp #1).</p>
	<pre><code>
%buffer size function used to determine the filter's dimensions
def get_buffer_sizes(filter):
    buffer_size = np.uint8(filter.shape[0])

    if is_1d_col(filter):
        buffer_size = np.uint8(filter.shape[1])

    half_buffer = np.uint8((buffer_size - 1) / 2)
    return buffer_size, half_buffer
	</code></pre>

	<pre><code>
%framing function
%the color_channels function separates an image into an interable channel component list
%the stacker function recombines the list of channels into a numpy array
def frame_image(image, buffer_size):
    buff = np.int(buffer_size)
    channels = [np.pad(channel, ((buff,buff), (buff,buff)), 'symmetric') for channel in color_channels(image)]
    return stacker(channels)
	</code></pre>
</div>

<div>
	<h3>Filtering</h3>
	<div style="float: right; padding: 20px">
		<img src="./images/low_frequencies_submarine.jpg" />
		<p style="font-size: 14px">Low frequency fish generated by my_imfilter function.</p>
	</div>
	<p>The filter process (<code>my_imfilter</code>) is the meat and potatoes of the program. A hideous for loop convolves each framed image pixel's neighbors with the provided filter for all three color channels. The setup phase ensures that each pixels neighbors, shall we say neighborhood, has similar dimensions as the provided function. The convolution output for each color channel is then recombined back into a numpy array. Please note that while I'm checking for 1-D filters, this is purely to ensure appropriate indexing and the overall process is treated the same regardless of the filter's dimensions. Numpy greatly simplified the process of taking the dot product, but of course I didn't realize that until re-reading the directions immediately prior to the deadline (womp womp #2). There is surely a more elegant way of handling the 1-D convolutions than what I've laid out], but nevertheless the separable convolutions seem to generate appropriate hybrid images.</p>
	<div style="clear:both">
		<pre><code>
%relevant filtering code
%result is assigned as the filtered channel's pixel value
neighbors = channel[i - HALF_BUFFER:i + HALF_BUFFER + 1, j - HALF_BUFFER:j + HALF_BUFFER + 1]
result = filter_neighborhood(filter, neighbors)

%relevant filter_neighborhood code
np.sum(np.multiply(filter, neighbors))
		</code></pre>
	</div>
</div>

<div>
	<h3>Composition</h3>
	<div style="float: right; padding: 20px">
		<img src="./images/high_frequencies_motorcycle.jpg" />
		<p style="font-size: 14px">High frequency motorcycle generated by subtracting low frequencies from base image.</p>
	</div>
	<p>After the setup and filtering phases the composition portion is relatively straightforward. The previously describes filtering process is leveraged to generate an image's low frequencies. The high frequencies are generated by subtracting the low frequencies from the high frequencies. Hybrid images are thus created by recombining the low and high frequencies of different images. Clipping has been greatly simplified by numpy, which I wish I had paid attention to initially instead of doing it myself (womp womp #3).</p>
	<div style="clear:both">
		<pre><code>
%relevant hybrid composition code
%first line subtracts the low frequencies from base to get high frequencies
%second line recombines high frequencies of one image with low frequencies of another image
high_frequencies = [img2_channel - low_frequencies2_channels[index] for index, img2_channel in enumerate(img2_channels)]
hybrid = [high_frequencies[index] + low_channel1 for index, low_channel1 in enumerate(low_frequencies1_channels)]
		</code></pre>
	</div>
</div>

<h3>Image Gallery</h3>

	<table border=1>
		<tr>
			<td>
				<img src="./images/low_frequencies_dog.jpg" width="25%"/>
				<img src="./images/high_frequencies_cat.jpg"  width="25%"/>
				<img src="./images/hybrid_image_scales_dog_cat.jpg" width="49%"/>
			</td>
		</tr>
		<tr>
			<td>
				<img src="./images/low_frequencies_bicycle.jpg" width="25%"/>
				<img src="./images/high_frequencies_motorcycle.jpg"  width="25%"/>
				<img src="./images/hybrid_image_scales_bicycle_motorcycle.jpg" width="49%"/>
			</td>
		</tr>
		<tr>
			<td>
				<img src="./images/low_frequencies_submarine.jpg" width="25%"/>
				<img src="./images/high_frequencies_fish.jpg"  width="25%"/>
				<img src="./images/hybrid_image_scales_submarine_fish.jpg" width="49%"/>
			</td>
		</tr>
		<tr>
			<td>
				<img src="./images/low_frequencies_marilyn.jpg" width="25%"/>
				<img src="./images/high_frequencies_einstein.jpg"  width="25%"/>
				<img src="./images/hybrid_image_scales_marilyn_einstein.jpg" width="49%"/>
			</td>
		</tr>
	</table>

<h3>Takeaways</h3>
<p>The hybrids that seemed to work best largely seemed to occur with images that were geometrically similar, but had dissimilar frequency compositions. For example, the fish/submarine hybrid didn't really turn out well combining the low frequency fish component with the high frequency submarine component despite similar geometric structure. Similarly, if the structural components were dissimilar the hybrid images were distracting and difficult to focus on. As a total aside, Apple is definitely filtering FaceTime videos somehow bc when I was showing some friends the frequencies were inverted (i.e. the dog-cat hybrid image was reversed).</p>
</body>
</html>
